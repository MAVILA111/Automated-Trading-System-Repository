In this first document, we will resume the principal ideas for Reinforcement Learning applied to finance.
The first approach is by the book Reinforcement Learning: An Introduction, Richard S. Sutton et al.
"...the distinction between problems and solution methods is very important in reinforcement learning"
" We use for RL the framework of dynamical system theory. It is part of optimal control of incompletely-known Markov decision processes."
Partially Observed Markov Decision Process POMPD
En un POMDP, un agente toma decisiones en un entorno donde el estado real no es completamente visible, sino que se obtiene a través de observaciones incompletas o ruidosas. Esto introduce incertidumbre en el proceso de toma de decisiones, ya que el agente debe inferir el estado actual en función de las observaciones y tomar decisiones óptimas teniendo en cuenta esta incertidumbre.
La teoría de POMDP proporciona un marco matemático para modelar estas situaciones y encontrar políticas óptimas que maximicen las recompensas esperadas a largo plazo. Un POMDP se define por los siguientes componentes:
Espacio de estados: representa los posibles estados en los que puede encontrarse el sistema. En el caso de la predicción del precio de los índices bursátiles, los estados podrían incluir diferentes niveles de precios, volatilidades u otros indicadores relevantes.
Espacio de acciones: son las decisiones que el agente puede tomar en cada estado. Por ejemplo, comprar, vender o mantener ciertos activos financieros.
Función de transición: describe la probabilidad de transición de un estado a otro dado una acción tomada por el agente. En el contexto de la predicción del precio de los índices bursátiles, esto podría modelar cómo el mercado reacciona a diferentes acciones y eventos.
Función de observación: indica la probabilidad de observar ciertos valores o señales dadas las acciones y los estados reales. En el caso de la predicción del precio de los índices bursátiles, podría representar la relación entre las observaciones del mercado, como los datos históricos o indicadores técnicos, y el estado real del mercado.
Función de recompensa: asigna una recompensa a cada acción tomada en función del estado en el que se encuentra el sistema. En el caso de la predicción del precio de los índices bursátiles, la función de recompensa podría estar relacionada con la precisión de las predicciones o el rendimiento de la estrategia de inversión.
El objetivo principal en un POMDP es encontrar una política óptima, que es una regla que indica qué acción tomar en cada estado observado para maximizar las recompensas esperadas a largo plazo. Sin embargo, la naturaleza parcialmente observada del problema introduce desafíos adicionales, ya que el agente debe inferir el estado actual basándose en las observaciones disponibles.
En el contexto de la predicción del precio de los índices bursátiles, un POMDP se puede utilizar para modelar la toma de decisiones de un agente que intenta predecir y aprovechar los movimientos del mercado.
